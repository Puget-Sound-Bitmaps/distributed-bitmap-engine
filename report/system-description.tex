\section{System Description}
DBIE is designed to run on Linux and its operability has been confirmed on
Ubuntu. The following sections detail various design choices made during the
implementation of DBIE.
%
\subsection{Basic Terminology}
A \term{node}---as it relates to our system---is a single computer with a separate
processor and disk from another node. A \term{process} is an instance of a program that is
being executed; each process contains the program code and the memory in use by
the current execution.

\subsection{Client Interface}
Our system's ``client'' is a database management system (DBMS). The DBMS
interfaces with our system through two functions: \code{PUT(k,v)} which adds a single
vector numbered \code{k} with value \code{v} (or replaces the value of vector \code{k}
with \code{v}), and \code{QUERY(q)} which returns the
results of a given query \code{q} on the index. In production, the DBMS could be a
full-featured product like SQLite, but we created a faux DBMS that provided
fine-grained control for our testing.
%
\subsection{System Architecture}

Our system is built using the \term{master-slave model}. There is one
master process to which the DBMS makes \code{PUT} and \code{GET} requests.
Upon receiving a \code{PUT} request, the master sends the vector to
two unique slave machines. The vector is saved on two machines so that if
one machine became inoperative, it is not lost.
\par
To satisfy a given query, the master node constructs a \term{query plan},
which specifies which slaves will help satisfy the query, and
in what order. The slaves work together to satisfy queries using vectors they
have, and return partial query results to the master. The master
collates the results from each slave and returns it to the DBMS.
The algorithms by which queries are satisfied are given in later sections.

%
\subsection{Interprocess Communication}
Distributed systems require additional methods of communication beyond those
used in single-process or systems. Communication between nodes
was accomplished using remote procedure calls (RPCs). An RPC is simply a
function call from one process to another \cite{tanenbaum1994}. RPCs are a
method of simplifying communication between nodes by hiding the networking
details and instead allowing developers to make function calls between nodes.
This helps to make the source code more readable and avoid mistakes in manually
opening and closing network connections. As they is currently implemented, the master node
resides on the same machine as the DBMS. The master and DBMS processes
communicate via Unix System V FIFO message queues. Requests are passed
as messages, and the results could be fed into a different queue but are
currently printed to standard out.
%
\subsection{Bitmap Compression}
In order to perform the necessary bitmap operations (\code{AND}s and
\code{OR}s), we utilized Chiu et al.'s bitmap engine which had been developed
by several students under the direction of Professor David Chiu. Chiu et al.'s
bitmap engine has implemented three compression and query methods: Byte aligned
Bitmap Code (BBC), Word aligned Hybrid Code (WAH), and Variable aligned Length
Code (VAL). Each of these three compression and query methods provides
substantial benefits over the use of uncompressed bitmap vectors. Of the three,
we chose to use WAH as it has been shown to be faster than BBC and it has a
simpler formatting than VAL. WAH allows compression of vectors such that all
information the vector gives is encoded, allows compressed vectors to be
queried. A complete description of WAH is given by \cite{wu2001}.
\subsection{Bitmap Vectors}
Slaves save vectors as files. The vector is broken up into 64-bit pieces, and
each piece is saved sequentially into the file.
%
\subsection{Consistent Hashing}
To determine on which two machines a given vector is located (or should be
located, if the identifier is new to the system) we use an algorithm known as
consistent hashing. Consistent hashing was introduced by
Karger~et~al.~\cite{karger1997} and can be understood conceptually as follows.
\par
Each slave machine is assigned a point on a circle where each point corresponds
to a value between \(0\) and \(2^{64} - 1\). The point for a machine with
identifier \(i\) is calculated as
\begin{equation*}
    \func{h}{i} \coloneqq \func{SHA1}{i} \bmod{2^{64}}.
\end{equation*}
To determine which machines (should) contain the vector \(k\), walk clockwise
from the point \(\func{h}{k}\) until reaching a machine node (the
\term{primary} node), and keep walking until reaching the next machine node
(the \term{backup} node). Once the primary and backup nodes have been
determined, return the set comprised of those two nodes. This procedure is
represented visually in Figure~\ref{fig:ring-hash-process} and formalized in
Algorithm~\ref{alg:consistent-hashing}.
%
\par
In our system, the consistent hashing structure is maintained as a red-black
tree, and determining the next node is given by the algorithms
\procedure{Tree-Succ} (Algorithm~\ref{alg:tree-succ}) and
\procedure{Recur-Succ} (Algorithm~\ref{alg:recur-succ} {\cite{bstPredecessorSuccessor}}). When the system is
initialized, each machine is assigned a nonnegative integer as an identifier.
Each node of the tree is associated with a machine in the system, and is
identified by the corresponding machine's id.
After the identifiers are assigned, they are inserted into the tree. Each node
has pointers to its left child, right child, and parent (accessible via the
functions \procedure{Left}, \procedure{Right} and \procedure{Par},
respectively). If a node does not have a parent or a child, that pointer will
hold the value \(null\). The algorithms for \procedure{Tree-Min} and
\procedure{Tree-Max} are given in \cite{cormen2009}
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Recur-Succ}{$tree, root, succ, key$}
            \If{$root = null$}
                \State \Return $succ$
            \ElsIf{$key = \func{h}{root.id}$}
                \State $succ \gets \Call{Right}{root}$
                \If{$succ = null$}
                    \State $succ \gets root$
                    \While{$\Call{Par}{succ} \neq null \land \func{h}{succ} < key$}
                        \State{$succ \gets \Call{Par}{succ}$}
                    \EndWhile
                \Else
                    \While{$\Call{Left}{succ} \neq null$}
                        \State{$succ \gets \Call{Left}{succ}$}
                    \EndWhile
                \EndIf
                \Return {$succ$}
            \ElsIf{$\func{h}{root.id} > key$}
                \State $left \gets \Call{Left}{root}$
                \State \Return \Call{Recur-Succ}{$tree, left, root, key$}
            \Else
                \State $right \gets \Call{Right}{root}$
                \State \Return \Call{Recur-Succ}{$tree, right, succ, key$}
            \EndIf
        \EndProcedure
    \end{algorithmic}
    \caption{Recursively Determined Successor Node}
    \label{alg:recur-succ}
\end{algorithm}
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Tree-Succ}{$tree, key$}
            \If{$\func{h}{key} \geq \func{h}{\Call{Tree-Max}{tree}.id}$}
                \State \Return \Call{Tree-Min}{$tree$}
            \Else
                \State $root \gets \Call{Root}{tree}$
                \State \Return \Call{Recur-Succ}{$tree, root, root, key$}
            \EndIf
        \EndProcedure
    \end{algorithmic}
    \caption{Successor Node}
    \label{alg:tree-succ}
\end{algorithm}
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Consistent-Hash}{$tree, key$}
            \State $m_0 \gets \Call{Tree-Succ}{tree, key}$
            \State $m_1 \gets \Call{Tree-Succ}{tree, m_0.id}$
            \State \Return $\set{m_0, m_1}$
        \EndProcedure
    \end{algorithmic}
    \caption{Consistent Hashing}
    \label{alg:consistent-hashing}
\end{algorithm}
%
\begin{figure}
    \centering
    \input{./imgs/ring-hash.tikz}
    \caption{Visualization of Ring Consistent Hashing}
    \label{fig:ring-hash-process}
\end{figure}
%
\subsection{Consistency}
After we had decided on a method for determining \emph{where} to store data,
we needed a method of ensuring that the data actually arrived at each node it
was suppose to. To do this, we implemented a system called Two-Phase Commit
(TPC) which ensures data consistency. It does this by checking that both slaves
are available: if so, the data is sent to both, if either is unavailable, the
unavailable node is handled and the process is restarted.
%
\subsection{Fault Tolerance}
The primary reason for using consistent hashing is for fault tolerance. While
waiting for messages from the DBMS, the master pings each slave to ensure that
each slave node is still online. If it doesn't hear back from a slave after a
reasonable length of time (one second in our system), it assumes that slave is
out of commission, and reallocates the vectors using \procedure{Reallocate}
(Algorithm~\ref{alg:reallocate}). In each node in the tree we store the identifiers of
the vectors on its associated machine, specifically for this purpose.
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Reallocate}{$tree, slave$}
            \State{$pred\gets\procedure{Tree-Pred}(tree, slave)$}
            \State{$succ\gets\procedure{Tree-Succ}(tree, slave)$}
            \State{$ssucc\gets\procedure{Tree-Succ}(tree, succ)$}
            \ForAll{$v\in (slave.vectors \cap succ.vectors)$}
                \State{$\procedure{Send-Vector}(succ, v, ssucc)$}
            \EndFor
            \ForAll{$w\in (slave.vectors \cap pred.vectors)$}
                \State{$\procedure{Send-Vector}(pred, w, succ)$}
            \EndFor
            \State{$\procedure{RB-Delete}(tree, slave)$}
        \EndProcedure
    \end{algorithmic}
    \caption{Reallocation}
    \label{alg:reallocate}
\end{algorithm}
%
\begin{figure}
    \centering
    \input{./imgs/reallocate.tikz}
    \caption[Visualization of Vector Reallocation]{%
        Visualization of Vector Reallocation\par
        In this example, node \(F\) has failed and the master had determined
        that we must reallocate vectors. The vectors that must be reallocated
        are those for which \(F\) was their primary location and those for
        which \(F\) was their backup location.
        First, vectors \(v_1\) and \(v_2\) had \(F\) as their primary location
        and \(B\) as their backup location (which will now become their primary
        location). Thus, \(B\) will send (in one message) copies of \(v_1\) and
        \(v_2\) to node \(C\) which will serve as the new backup location.
        Second, vectors \(v_4\) and \(v_5\) will be sent (in one message) from
        \(A\) (their primary location) to \(B\) which will take over as backup
        for \(F\).
        After this, the system will once again have two copies of each vector.
    }
    \label{fig:vector-reallocation}
\end{figure}
%
\par
In our implementation we utilized algorithms provided by \cite{cormen2009}
to find the predecessor of a slave (\procedure{Tree-Pred}), to find the
successor of a slave (\procedure{Tree-Succ}), and to delete a slave from the
tree (\procedure{RB-Delete}). We also made use of an RPC,
\(\func{\procedure{Send-Vector}}{s_1, k, s_2}\),
that we wrote which makes slave \(s_1\) send the vector \(k\) to slave \(s_2\).
\par
As a result of using consistent hashing, we know that the failed slave's backup
vectors are the primary vectors of its predecessor. The successor must now
maintain copies of the predecessor's vectors, so the master tells the
predecessor to communicate its primary vectors to the successor. Likewise,
since the successor node now holds the slave's primary vectors alongside its
own, it must send the slave's primary vectors to its successor as a backup.
Consistent hashing is preferable to using a hash-mod-n technique
because potentially every every single vector would have to be remapped if
the system changed,
requiring significantly more message passing than the ones
used in \procedure{Reallocate}. This is because for
hash-mod-n an algorithms whose hash function modulus is the
number of machines, when a slave node is removed, the primary locations of
potentially every vector changes, thus requiring us to remap every single
vector in the worst case \cite{kleppman2017}. While hash-mod-n requires \(\func{O}{K}\)
remappings, where \(K\) is the number of keys, consistent hashing only requires
\(\func{O}{K/n}\) remappings.
\cite{karger1997}.
%
\subsection{Queries}
Our system is designed to handle range queries. An example range query is
\code{R:[0,3]\&[5,7]}. Such a query might correspond to a range query in SQL.
The query given above requires the following steps:
\begin{algorithmic}
    \State $w_0 \gets v_0 \lor v_1 \lor v_2 \lor v_3$
    \State $w_1 \gets v_5 \lor v_6 \lor v_7$
    \State \Return $w_0 \land w_1$
\end{algorithmic}
To carry out the query, we first create a plan that tells us which slave nodes
to visit.
\subsection{Query Planning}
Our query planning algorithm (Algorithm~\ref{alg:query-planning}) takes two
inputs. The first input, \(tree\), is the red-black tree used in consistent
hashing to determine which two machines contain a given vector identifier. The
second input,
\[R = \set{\tuple{v_0, v_1}, \tuple{v_1, v_2}, \ldots, \tuple{v_n, v_m}},\]
holds the ranges of the query as an ordered set of tuples where
\(v_i \leq v_j\) and \(0 \leq i \leq j \leq n \leq m\), and each \(v_i\) is a
vector identifier. Sorting is performed so that, in each portion of the query,
slaves do not have to be visited more than once, making query processing
a linear operation. Because \(\lor\) is communative, it does not matter which
order the vectors are \code{OR}ed together within the subquery. The \(swap\) variable is used
to introduce variance in what machines are visited to retrieve a given vector, so
that replication of vectors can be used to balance the work of the query among
primary and secondary machines.\par The return value is
a set $Q$ of \term{subqueries}, where each subquery comprises one or more tuples
of the form \(\tuple{machine\_id,vector\_id}\), which will be used in
the query execution algorithms to determine which slaves to visit
and which vectors to obtain.
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Range-Query-Plan}{$tree, R$}
            \State{$swap \gets false$}
            \State{$paths \gets \emptyset$}
            \ForAll{$r \in R$}
                \State{$subpaths \gets \emptyset$}
                \For{$k \gets r_0, r_1$}
                    \State{$machines \gets \Call{Consistent-Hash}{tree, k}$}
                    \State{$tuple \gets \emptyset$}
                    \If{$swap$}
                        \State{$tuple \gets tuple \cup \set{machines_0}$}
                    \Else
                        \State{$tuple \gets tuple \cup \set{machines_1}$}
                    \EndIf
                    \State{$tuple \gets tuple \cup \set{k}$}
                    \State{$subpaths \gets subpaths \cup tuple$}
                \EndFor
                \State{Sort $subpaths$ on $machine.id$}
                \State{$paths \gets paths \cup subpath$}
                \State{$swap \gets \neg swap$}
            \EndFor
            \Return $paths$
        \EndProcedure
    \end{algorithmic}
    \caption{Query Planning}
    \label{alg:query-planning}
\end{algorithm}
%
\subsection{Query Execution}
Execution of queries received by the master is handled using
Algorithm~\ref{alg:master-query-root} which first plans out the query using
Algorithm~\ref{alg:query-planning} and then delegates each subquery to its slaves using
Algorithm~\ref{alg:slave-subquery}.\par
Algorithm~\ref{alg:slave-subquery} is an RPC takes a machine identifier, denoting
the slave that the function is run on, and a set of tuples comprising a subquery.
The slave iterates over the tuples referencing vectors it contains, and \code{OR}s
them together using WAH, as denoted by \(\lor\). Once the slave has operated on all
requested vectors it holds, it makes an RPC to the slave in the subsequent tuple,
recursively satisfying the remainder of the subquery.\par
Algorithm~\ref{alg:master-query-root} takes a complete query, dividing the work
among the slaves, and \code{AND}ing the results of each subquery together,
returning the result to the DBMS.
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Master-Query-Root}{$Q$}
            \State $R \gets \emptyset$
            \ForAll{$q \in Q$}
                \Comment{Delegate subqueries.}
                \State $slave\_id \gets {q_0}_0$
                \State $r \gets \Call{Range-SubQuery}{slave\_id, q}$
                \State $R \gets R \cup \set{r}$
            \EndFor
            \State $v \gets \vec{1}$
            \ForAll{$w \in R$}
                \Comment{\code{AND} the results.}
                \State $v \gets v \land w$
            \EndFor
            \State \Return $v$
        \EndProcedure
    \end{algorithmic}
    \caption{Master Query Root}
    \label{alg:master-query-root}
\end{algorithm}
%
\begin{algorithm}
    \begin{algorithmic}
        \Procedure{Range-SubQuery}{$machine\_id,q$}
            \State $r \gets \vec{0}$
            \ForAll{$tuple \in q$}
                \If{$tuple_0 = self$}
                    \State $r \gets r \lor \Call{Retrieve-Vector}{tuple_1}$
                    \State $q \gets q \setminus \set{tuple}$
                \Else
                    \State $s \gets \Call{Range-SubQuery}{tuple_0, q}$
                    \State $r \gets r \lor s$
                    \State \Break
                \EndIf
            \EndFor
            \State \Return $r$
        \EndProcedure
    \end{algorithmic}
    \caption{Slave subquery}
    \label{alg:slave-subquery}
\end{algorithm}
%
\subsection{Implementation and Testing Notes}
Due to its speed and established usage in systems like ours, such as Redis
\cite{redis}, we settled on writing our system primarily in the C programming
language. In addition to C, Python~3 was used to script the production of
testing data, which we obtained from the TPC-C data repository. Python~3 was
also used in collaboration with bash to produce startup scripts that
facilitated automatic testing of the system. In order to create our RPCs, we
had to specify the types of RPCs being used in the ONC+ RPC language which is
an extension of the eXternal Data Representation (XDR) language developed in
the mid 1980s by Sun Microsystems.
